## Processing
n_ver_load: 1
n_step_load: 20000
n_save_ver: 1
n_ver_picks: 1

template_ver: 1
vel_model_ver: 1
process_days_ver: 1 
## Put the list of days you want to process in file $ProjectName$_process_days_list_ver_%d.txt

## Note: the default settings have some redundance and averaging built in, but speeds can be improved in a few ways.
## To speed up processing, can increase step (e.g. step: 5 - 10), and use only one grid during location refinement step (use_only_one_grid: True)
## To have higher accuracy, decrease step (e.g., step: 2), and average over all grids (use_only_one_grid: False)

## Additional ways to speed up code are decreasing n_rand_query to loose some spatial resolution, and also increase thresh to reduce number of sources,
## which reduces post-processing costs. The simplest way to process long intervals of time is to call process_continuous_days.py on many different
## input days (specified by the system argument argv[1] passed to the script: e.g., python process_continuous_days.py $n$ for any integer $n$, 
## which selects the nth day given in $ProjectName$_process_days_list_ver_1.txt), so that many parallel days can be submitted to run on individual cores.


offset_increment: 500 ## This only effects the choice of pick files used when submitting multiple jobs such as jobarrays
n_rand_query: 112000 ## This is the number of spatial points to re-locate initial source locations with higher resolution (an efficient implementation)
n_query_grid: 10000 ## This is the number of spatial points to save initial space-time continuous outputs over

thresh: 0.3 # Threshold to declare detection (previously, ~0.15 - 0.2 was effective, but recent changes to synthetic data tend to make higher thresholds also effective (e.g., even ~0.5 - 0.8 may be usable))
thresh_assoc: 0.3 # Threshold to declare src-arrival association
spr: 1 # Sampling rate to save temporal predictions (may still cause some issues if this is changed from 1 s for now)
tc_win: 5.0 # Temporal window (s) to link events in Local Marching
sp_win: 15000.0 # Distance (m) to link events in Local Marching. Generally both of these on the order of training kernel sizes
break_win: 15.0 # Temporal window to find disjoint groups of sources, 
## so can run Local Marching without memory issues.
spr_picks: 1 # Assumed sampling rate of picks (e.g., spr_picks = 1 means absolute time, spr_picks = 100 means 100 Hz sampling rate). Note, previous default was spr_picks = 100.
## (can be 1 if absolute times are used for pick time values)

d_win: 0.25 ## Lat and lon window to re-locate initial source detetections with refined sampling over
d_win_depth: 10000.0 ## Depth window to re-locate initial source detetections with refined sampling over
dx_depth: 50.0 ## Depth resolution to locate events with travel time based re-location (unused, if using differential evolution location)

step: 2.0 ## (for more efficient processing choose 5 s) Temporal step size to predict source outputs over, and stack overlapping portions (must be less than default source window prediction is 10 s)
step_abs: 1.0 ## Time resolution to save the initial space-time predictions

cost_value: 2.5 # If use expanded competitve assignment, then this is the fixed cost applied per source
## when optimizing joint source-arrival assignments between nearby sources. The value is in terms of the 
## `sum' over the predicted source-arrival assignment for each pick. Ideally could make this number more
## adpative, potentially with number of stations or number of possible observing picks for each event. 

device: 'cpu' ## Right now, this isn't updated to work with cuda, since
## the necessary variables do not have .to(device) at the right places

use_only_one_grid: False # (for more efficient processing choose True) If True, processing can be ~2 - 4 times faster, while loosing a bit of accuracy gained from averaging over multiple spatial grids
compute_magnitudes: False # Uses local magnitude and will be un-calibrated by default, or can calibrate for a particular study site

process_known_events: False ## Only process times of known events in a reference catalog (default is USGS)
use_expanded_competitive_assignment: True ## This is advised, but the code is complex to avoid both memory issues and handling nearby or overlapping sources
use_differential_evolution_location: True # otherwise use particle swarm to locate, which tends to converge less well (especially in depth)
